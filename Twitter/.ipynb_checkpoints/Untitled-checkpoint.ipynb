{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage import exposure, img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_image_path = './pro_img/'\n",
    "\n",
    "\n",
    "def inspect_dataset(df_data):\n",
    "    \"\"\"pytoho\n",
    "        查看加载的数据基本信息\n",
    "    \"\"\"\n",
    "    print('数据集基本信息：')\n",
    "    print(df_data.info())\n",
    "    print('数据集有{}行，{}列'.format(df_data.shape[0], df_data.shape[1]))\n",
    "    print('数据预览:')\n",
    "    print(df_data.head())\n",
    "\n",
    "\n",
    "def check_profile_image(img_link):\n",
    "    \"\"\"\n",
    "        判断头像图片链接是否有效\n",
    "        如果有效，下载到本地，并且返回保存路径\n",
    "    \"\"\"\n",
    "    save_image_path = ''\n",
    "    # 有效的图片扩展名\n",
    "    valid_img_ext_lst = ['.jpeg', '.png', '.jpg']\n",
    "\n",
    "    try:\n",
    "        img_data = io.imread(img_link)\n",
    "        image_name = img_link.rsplit('/')[-1]\n",
    "        if any(valid_img_ext in image_name.lower() for valid_img_ext in valid_img_ext_lst):\n",
    "            # 确保图片文件包含有效的扩展名\n",
    "            save_image_path = os.path.join(profile_image_path, image_name)\n",
    "            io.imsave(save_image_path, img_data)\n",
    "    except:\n",
    "        print('头像链接 {} 无效'.format(img_link))\n",
    "\n",
    "    return save_image_path\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        清洗文本数据\n",
    "    \"\"\"\n",
    "    # just in case\n",
    "    text = text.lower()\n",
    "\n",
    "    # 去除特殊字符\n",
    "    text = re.sub('\\s\\W', ' ', text)\n",
    "    text = re.sub('\\W\\s', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_train_test(df_data, size=0.8):\n",
    "    \"\"\"\n",
    "        分割训练集和测试集\n",
    "    \"\"\"\n",
    "    # 为保证每个类中的数据能在训练集中和测试集中的比例相同，所以需要依次对每个类进行处理\n",
    "    df_train = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    labels = [0, 1]\n",
    "    for label in labels:\n",
    "        # 找出gender的记录\n",
    "        text_df_w_label = df_data[df_data['label'] == label]\n",
    "        # 重新设置索引，保证每个类的记录是从0开始索引，方便之后的拆分\n",
    "        text_df_w_label = text_df_w_label.reset_index()\n",
    "\n",
    "        # 默认按80%训练集，20%测试集分割\n",
    "        # 这里为了简化操作，取前80%放到训练集中，后20%放到测试集中\n",
    "        # 当然也可以随机拆分80%，20%（尝试实现下DataFrame中的随机拆分）\n",
    "\n",
    "        # 该类数据的行数\n",
    "        n_lines = text_df_w_label.shape[0]\n",
    "        split_line_no = math.floor(n_lines * size)\n",
    "        text_df_w_label_train = text_df_w_label.iloc[:split_line_no, :]\n",
    "        text_df_w_label_test = text_df_w_label.iloc[split_line_no:, :]\n",
    "\n",
    "        # 放入整体训练集，测试集中\n",
    "        df_train = df_train.append(text_df_w_label_train)\n",
    "        df_test = df_test.append(text_df_w_label_test)\n",
    "\n",
    "    df_train = df_train.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def get_word_list_from_data(text_s):\n",
    "    \"\"\"\n",
    "        将数据集中的单词放入到一个列表中\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    for _, text in text_s.iteritems():\n",
    "        word_list += text.split(' ')\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def proc_text(text):\n",
    "    \"\"\"\n",
    "        分词+去除停用词\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "def extract_tf_idf(text_s, text_collection, common_words_freqs):\n",
    "    \"\"\"\n",
    "        提取tf-idf特征\n",
    "    \"\"\"\n",
    "    # 这里只选择TF-IDF特征作为例子\n",
    "    # 可考虑使用词频或其他文本特征作为额外的特征\n",
    "\n",
    "    n_sample = text_s.shape[0]\n",
    "    n_feat = len(common_words_freqs)\n",
    "\n",
    "    common_words = [word for word, _ in common_words_freqs]\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取tf-idf特征...')\n",
    "    for i, text in text_s.iteritems():\n",
    "        feat_vec = []\n",
    "        for word in common_words:\n",
    "            if word in text:\n",
    "                # 如果在高频词中，计算TF-IDF值\n",
    "                tf_idf_val = text_collection.tf_idf(word, text)\n",
    "            else:\n",
    "                tf_idf_val = 0\n",
    "\n",
    "            feat_vec.append(tf_idf_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def hex_to_rgb(value):\n",
    "    \"\"\"\n",
    "        十六进制颜色码转换为RGB值\n",
    "    \"\"\"\n",
    "    rgb_list = list(int(value[i:i + 2], 16) for i in range(0, 6, 2))\n",
    "    return rgb_list\n",
    "\n",
    "\n",
    "def extract_rgb_feat(hex_color_s):\n",
    "    \"\"\"\n",
    "         从十六进制颜色码中提取RGB值作为特征\n",
    "    \"\"\"\n",
    "    n_sample = hex_color_s.shape[0]\n",
    "    n_feat = 3\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取RGB特征...')\n",
    "    for i, hex_val in hex_color_s.iteritems():\n",
    "        feat_vec = hex_to_rgb(hex_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def extract_rgb_hist_feat(img_path_s):\n",
    "    \"\"\"\n",
    "        从图像中提取RGB直方图特征\n",
    "    \"\"\"\n",
    "    n_sample = img_path_s.shape[0]\n",
    "    n_bins = 100    # 每个通道bin的个数\n",
    "    n_feat = n_bins * 3\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取RGB直方图特征...')\n",
    "    for i, img_path in img_path_s.iteritems():\n",
    "        # 加载图像\n",
    "        img_data = io.imread(img_path)\n",
    "        img_data = img_as_float(img_data)\n",
    "\n",
    "        if img_data.ndim == 3:\n",
    "            # 3个通道\n",
    "            hist_r, _ = exposure.histogram(img_data[:, :, 0], nbins=n_bins)\n",
    "            hist_g, _ = exposure.histogram(img_data[:, :, 1], nbins=n_bins)\n",
    "            hist_b, _ = exposure.histogram(img_data[:, :, 2], nbins=n_bins)\n",
    "        else:\n",
    "            # 2个通道\n",
    "            hist, _ = exposure.histogram(img_data, nbins=n_bins)\n",
    "            hist_r = hist.copy()\n",
    "            hist_g = hist.copy()\n",
    "            hist_b = hist.copy()\n",
    "\n",
    "        feat_vec = np.concatenate((hist_r, hist_b, hist_g))\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip(zip_filepath, dest_path):\n",
    "    \"\"\"\n",
    "        解压zip文件\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filepath) as zf:\n",
    "        zf.extractall(path=dest_path)\n",
    "\n",
    "\n",
    "def get_dataset_filename(zip_filepath):\n",
    "    \"\"\"\n",
    "        获取数据库文件名\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filepath) as zf:\n",
    "        return zf.namelist()[0]\n",
    "\n",
    "\n",
    "def cal_acc(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "        计算准确率\n",
    "    \"\"\"\n",
    "    n_total = len(true_labels)\n",
    "    correct_list = [true_labels[i] == pred_labels[i] for i in range(n_total)]\n",
    "\n",
    "    acc = sum(correct_list) / n_total\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './dataset'  # 数据集路径\n",
    "zip_filename = 'twitter-user-gender-classification.zip'  # zip文件名\n",
    "zip_filepath = os.path.join(dataset_path, zip_filename)  # zip文件路径\n",
    "cln_datapath = './cln_data.csv'     # 清洗好的数据路径\n",
    "# 是否第一次运行\n",
    "is_first_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                                                                                               zx\n",
    "        主函数\n",
    "    \"\"\"\n",
    "dataset_filename = get_dataset_filename(zip_filepath)  # 数据集文件名（在zip中）\n",
    "dataset_filepath = os.path.join(dataset_path, dataset_filename)  # 数据集文件路径\n",
    "if is_first_run:\n",
    "    print('解压zip...')\n",
    "    unzip(zip_filepath, dataset_path)\n",
    "    print('完成.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集基本信息：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 6 columns):\n",
      "gender           19953 non-null object\n",
      "description      16306 non-null object\n",
      "link_color       20050 non-null object\n",
      "profileimage     20050 non-null object\n",
      "sidebar_color    20050 non-null object\n",
      "text             20050 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 939.9+ KB\n",
      "None\n",
      "数据集有20050行，6列\n",
      "数据预览:\n",
      "   gender                                        description link_color  \\\n",
      "0    male                              i sing my own rhythm.     08C2C2   \n",
      "1    male  I'm the author of novels filled with family dr...     0084B4   \n",
      "2    male                louis whining and squealing and all     ABB8C2   \n",
      "3    male  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...     0084B4   \n",
      "4  female  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...     3B94D9   \n",
      "\n",
      "                                        profileimage sidebar_color  \\\n",
      "0  https://pbs.twimg.com/profile_images/414342229...        FFFFFF   \n",
      "1  https://pbs.twimg.com/profile_images/539604221...        C0DEED   \n",
      "2  https://pbs.twimg.com/profile_images/657330418...        C0DEED   \n",
      "3  https://pbs.twimg.com/profile_images/259703936...        C0DEED   \n",
      "4  https://pbs.twimg.com/profile_images/564094871...             0   \n",
      "\n",
      "                                                text  \n",
      "0  Robbie E Responds To Critics After Win Against...  \n",
      "1  ÛÏIt felt like they were my friends and I was...  \n",
      "2  i absolutely adore when louis starts the songs...  \n",
      "3  Hi @JordanSpieth - Looking at the url - do you...  \n",
      "4  Watching Neighbours on Sky+ catching up with t...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(dataset_filepath, encoding='latin1',\n",
    "                           usecols=['gender', 'description', 'link_color',\n",
    "                                    'profileimage', 'sidebar_color', 'text'])\n",
    "inspect_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗文本数据...\n"
     ]
    }
   ],
   "source": [
    "filtered_data = data[(data['gender'] == 'male') | (data['gender'] == 'female')]\n",
    "\n",
    "        # 2.2 过滤掉 'description' 列为空的数据\n",
    "filtered_data = filtered_data.dropna(subset=['description'])\n",
    "\n",
    "        # 2.3 过滤掉 'link_color' 列和 'sidebar_color' 列非法的16进制数据\n",
    "filtered_data = filtered_data[filtered_data['link_color'].str.len() == 6]\n",
    "filtered_data = filtered_data[filtered_data['sidebar_color'].str.len() == 6]\n",
    "\n",
    "        # 2.4 清洗文本数据\n",
    "print('清洗文本数据...')\n",
    "cln_desc = filtered_data['description'].apply(clean_text)\n",
    "cln_text = filtered_data['text'].apply(clean_text)\n",
    "filtered_data['cln_desc'] = cln_desc\n",
    "filtered_data['cln_text'] = cln_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "female    3961\n",
      "male      3908\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "  # 保存处理好的数据\n",
    "filtered_data.to_csv(cln_datapath, index=False, encoding='latin1')\n",
    "   # 读取处理好的数据\n",
    "clean_data = pd.read_csv(cln_datapath, encoding='latin1',\n",
    "                             usecols=['gender', 'cln_desc', 'cln_text',\n",
    "                                      'link_color', 'sidebar_color'])\n",
    "\n",
    "    # 查看label的分布\n",
    "print(clean_data.groupby('gender').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中各类的数据个数： label\n",
      "0.0    3126\n",
      "1.0    3168\n",
      "dtype: int64\n",
      "测试集中各类的数据个数： label\n",
      "0.0    782\n",
      "1.0    793\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "clean_data.loc[clean_data['gender'] == 'male', 'label'] = 0\n",
    "clean_data.loc[clean_data['gender'] == 'female', 'label'] = 1\n",
    "\n",
    "    # 3. 分割数据集\n",
    "    # 分词 去除停用词\n",
    "proc_desc_s = clean_data['cln_desc'].apply(proc_text)\n",
    "clean_data['desc_words'] = proc_desc_s\n",
    "proc_text_s = clean_data['cln_text'].apply(proc_text)\n",
    "clean_data['text_words'] = proc_text_s\n",
    "df_train, df_test = split_train_test(clean_data)\n",
    "    # 查看训练集测试集基本信息\n",
    "print('训练集中各类的数据个数：', df_train.groupby('label').size())\n",
    "print('测试集中各类的数据个数：', df_test.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本特征提取：\n",
      "统计description词频...\n",
      "[('', 34), ('cos even old girl best friend still mum', 18), ('baby could enough', 10), ('ï _ _ùi really love one direction _ù _ ï', 7), ('love', 7), ('twitter bot tweeting english posts francoisgoube ceo cogniteev oncrawl docidohq', 6), ('_ ü sometime last night _ ü r5 world _ ü four 1d made _ ü nialler _ ü', 6), ('like secret little rendezvous perfect j aime dylan et zayn', 6), ('czech adult model pornographic actress 42 inch natural breasts currently livin london follow retweet every day luv u', 5), ('mapping geek ecologist data lover use twitterfeed autotweet uk wildlife flickr pics use twitter widgets html expanded pics practice id', 5), ('run', 5), ('datpiff promo hotnewhiphop promo reverbnation promo soundcloud best net', 5), ('happy', 4), ('best bio ever fuck', 4), ('must 18 view website follow mention retweets check hottest girls _á http co x1mesztesn âé', 4), ('ashley madison open looking turns profile captions member information', 4), ('greek directioner zquad mixer ª hope one member one direction little mix zayn malik follow ª', 4), ('going gets tough tough get going', 3), ('hi', 3), ('birmingham', 3), ('10 month thats like 50 year', 3), ('êï üö 18 ï soonrmt ï legend ï introvert ï ptosis japan ï korea ï ph louis_tomlinson everything teejaymarquez babe ig ohmyjeney', 3), ('professional box mover stuffer ohio boy master couch potato around pain rear end please meet yo0u', 3), ('ï', 3), ('bio', 3), ('ª ü', 3), ('know', 3), ('even night changes never change love one direction ªá ö_ ûó õ ò ò òä õ_ õ¼ ò òã òü õ¼', 3), ('die legend', 3), ('sometimes say stuff', 3), ('poet writer bss 4korna outtaorbit mixedchemistry stardommag101 hmg cloudz', 3), ('woods', 3), ('interviewer host bookings event coverage email taniaonthescene gmail com ftlornb radio tues 10p blogger jackthriller pix11news', 3), ('17', 3), ('ask wont bite well hard anyway lol', 3), ('saya adalah saya kamu adalah kamu penggemar setia mufc', 3), ('h king queen', 3), ('wait', 3), ('tronnor', 3), ('secret little rendez vous', 3), ('head chef chez bruce loves cars bicycles 90 food', 3), ('admiro las personas que con el corazì_n roto pueden sonreì_r decir estoy bien vanelover shawnmendes 29 03 15 justinbieber 02 07 15 allisimpson 20 09 15', 3), ('news 7 7 24 24', 3), ('nothing', 3), ('angel could _ä ü', 2), ('maker conceptor creative developer 0xbac3a9bd conception geek dev agile neutralite opensource privacy copyright', 2), ('quit paying expensive medical visits fake fixes eliminate frustrating nagging constantly repeating low back pain', 2), ('signed', 2), ('brain bodily organ starts working moment awake stop get class', 2), ('entì estìá bem 18 sporting clube de portugal 1906', 2)]\n"
     ]
    }
   ],
   "source": [
    "    # 4. 特征工程\n",
    "    # 4.1 训练数据特征提取\n",
    "print('训练样本特征提取：')\n",
    "    # 4.1.1 文本数据\n",
    "    # description数据\n",
    "print('统计description词频...')\n",
    "n = 50\n",
    "from nltk import FreqDist\n",
    "freq_dist = FreqDist(proc_desc_s)\n",
    "desc_most_common_words = freq_dist.most_common(n)\n",
    "print(desc_most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计description词频...\n",
      "descriptino中出现最多的50个词是：\n",
      "_: 523次\n",
      "co: 483次\n",
      "û: 464次\n",
      "ü: 462次\n",
      "love: 458次\n",
      "life: 353次\n",
      "http: 294次\n",
      "fan: 216次\n",
      "music: 193次\n",
      "writer: 192次\n",
      "lover: 182次\n",
      "like: 182次\n",
      "ï: 173次\n",
      "one: 153次\n",
      "world: 150次\n",
      "follow: 146次\n",
      "ig: 145次\n",
      "god: 143次\n",
      "êû: 137次\n",
      "live: 136次\n",
      "time: 134次\n",
      "ª: 133次\n",
      "sports: 128次\n",
      "com: 123次\n",
      "instagram: 123次\n",
      "https: 116次\n",
      "things: 109次\n",
      "make: 109次\n",
      "best: 109次\n",
      "girl: 106次\n",
      "2: 105次\n",
      "family: 104次\n",
      "good: 104次\n",
      "artist: 101次\n",
      "snapchat: 99次\n",
      "author: 98次\n",
      "people: 97次\n",
      "man: 96次\n",
      "ªá: 95次\n",
      "get: 95次\n",
      "new: 95次\n",
      "twitter: 94次\n",
      "blogger: 93次\n",
      "18: 92次\n",
      "student: 91次\n",
      "im: 90次\n",
      "never: 89次\n",
      "business: 87次\n",
      "4: 87次\n",
      "always: 87次\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('统计description词频...')\n",
    "n_desc_common_words = 50\n",
    "desc_words_in_train = get_word_list_from_data(df_train['desc_words'])\n",
    "fdisk = nltk.FreqDist(desc_words_in_train)\n",
    "desc_common_words_freqs = fdisk.most_common(n_desc_common_words)\n",
    "print('descriptino中出现最多的{}个词是：'.format(n_desc_common_words))\n",
    "for word, count in desc_common_words_freqs:\n",
    "    print('{}: {}次'.format(word, count))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import TextCollection\n",
    "word = 'That'\n",
    "tc = TextCollection([text1, text2, text3,text4, text5])\n",
    "tf_idf_val = tc.tf_idf(word, new_text)\n",
    "print('{}的TF-IDF值为：{}'.format(word, tf_idf_val))\n",
    "for word, count in desc_common_words_freqs:\n",
    "    print('{}: {}次'.format(word, count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
